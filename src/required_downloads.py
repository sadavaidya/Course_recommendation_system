from nltk.tokenize import word_tokenize

# Sample text
text = "This is a test sentence to check tokenization."

# Tokenize
tokens = word_tokenize(text)

# Output the result
print("Tokenized text:", tokens)
